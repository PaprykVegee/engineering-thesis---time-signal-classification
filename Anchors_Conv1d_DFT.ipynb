{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0cf5830-4002-4118-b2cc-ca2aa8062d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 directories and 1 file in C:\\Users\\patry\\OneDrive\\Pulpit\\testv2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nptdms import TdmsFile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the directory containing TDMS files\n",
    "tdms_dir = r'C:\\Users\\patry\\OneDrive\\Pulpit\\testv2'\n",
    "\n",
    "# List to store paths of TDMS files\n",
    "tdms_list = []\n",
    "\n",
    "# Walk through the directory and its subdirectories\n",
    "for root, dirs, files in os.walk(tdms_dir):\n",
    "    print(f\"There are {len(dirs)} directories and {len(files)} file in {root}\")\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".tdms\"):\n",
    "            # If the file ends with '.tdms', add its full path to the list\n",
    "            tdms_list.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "007833ef-c3ab-4d05-903d-3dfa4ff93938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Create an empty list to store data\n",
    "data_list = []\n",
    "\n",
    "# Open tdms files and create a list with labels\n",
    "for tdms_file_path in tdms_list:\n",
    "    tdms_file = TdmsFile.read(tdms_file_path)\n",
    "    first_letter = os.path.basename(tdms_file_path)[0]\n",
    "    \n",
    "    for group in tdms_file.groups():\n",
    "        for channel in group.channels():\n",
    "            data_len = np.shape(channel.data)[0]\n",
    "            # Check conditions and append data to list\n",
    "            if (data_len <= 1000000 and data_len >= 10000 and first_letter != 'z'):\n",
    "                data_list.append({\n",
    "                    #'anchor_ids': group.name,\n",
    "                    'class': first_letter,\n",
    "                    #'driveway': channel.name,\n",
    "                    'excitation': channel.data,\n",
    "                    'type_id': 1 if first_letter == 'd' else 0\n",
    "                })\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df_date = df.loc[df.index % 2 == 0]\n",
    "df_magnitude = df.loc[df.index % 2 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d89dff47-15aa-4d0b-b5d5-3132d0a084b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_14260\\44183370.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_magnitude['excitation'] = df_magnitude['excitation'].apply(lambda x: x if isinstance(x, (list, np.ndarray)) else [x])\n",
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_14260\\44183370.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_magnitude['excitation'] = padded_signals\n"
     ]
    }
   ],
   "source": [
    "# Upewnij się, że wszystkie wartości w kolumnie 'excitation' są listami lub tablicami\n",
    "df_magnitude['excitation'] = df_magnitude['excitation'].apply(lambda x: x if isinstance(x, (list, np.ndarray)) else [x])\n",
    "\n",
    "# Znajdź maksymalną długość danych w kolumnie 'excitation'\n",
    "max_length = max(df_magnitude['excitation'].apply(len))\n",
    "print(max_length)\n",
    "# Zero-pad each signal to the maximum length\n",
    "padded_signals = df_magnitude['excitation'].apply(lambda x: np.pad(x, (0, max_length - len(x)), 'mean')).values.tolist()\n",
    "\n",
    "df_magnitude['excitation'] = padded_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfc089f3-a16f-43e5-aaf9-09c6fdcc4ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_magnitude['excitation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9ee49d7-9283-476b-9b79-733b51d6864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_14260\\3638390617.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_magnitude['excitation'] = [list(row) for row in normalized_data]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standarizing data\n",
    "data = np.array(df_magnitude['excitation'].tolist())\n",
    "#print(len(data[0]))\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "normalized_data = scaler.transform(data)\n",
    "\n",
    "df_magnitude['excitation'] = [list(row) for row in normalized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba0aba07-dc46-4953-91f6-6ecb83388059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_magnitude['excitation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "063b9b6a-bcc3-4c4f-bdbc-807dae2fd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "class Anchors_data(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def _compute_dft(self, signal):\n",
    "        # Convert the excitation list of lists to a numpy array\n",
    "        excitation_array = np.array(signal)\n",
    "        \n",
    "        # Ensure excitation_array is one-dimensional\n",
    "        excitation_array = excitation_array.flatten()\n",
    "\n",
    "        # Compute DFT\n",
    "        dft_data = np.fft.fft(excitation_array)\n",
    "        \n",
    "        # Extract the positive frequencies, excluding the zero frequency component\n",
    "        positive_frequencies = dft_data[2:(len(dft_data) // 2)]\n",
    "        \n",
    "        return positive_frequencies.real  # Return only the real part\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.df['excitation'].iloc[idx]\n",
    "        dft_data = self._compute_dft(signal)\n",
    "        \n",
    "        type_id = self.df['type_id'].iloc[idx]\n",
    "        return torch.tensor(dft_data, dtype=torch.float32).unsqueeze(dim=0), torch.tensor(type_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1655eeba-764a-4050-812a-5a7d222440d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming df_magnitude is already defined\n",
    "train_df, test_df = train_test_split(df_magnitude, test_size=0.3, random_state=42)\n",
    "\n",
    "train_data = Anchors_data(train_df)\n",
    "test_data = Anchors_data(test_df)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f93e1b89-9cf5-498d-9d76-5a21a43bacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 379998]), torch.Size([8]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].size(), next(iter(train_dataloader))[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1105e91-e870-47ac-8cb0-93a41ecce50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1           [-1, 16, 499998]              64\n",
      "              ReLU-2           [-1, 16, 499998]               0\n",
      "         MaxPool1d-3           [-1, 16, 249999]               0\n",
      "            Conv1d-4           [-1, 32, 249999]           1,568\n",
      "              ReLU-5           [-1, 32, 249999]               0\n",
      "         MaxPool1d-6           [-1, 32, 124999]               0\n",
      "            Conv1d-7           [-1, 64, 124999]           6,208\n",
      "              ReLU-8           [-1, 64, 124999]               0\n",
      "         MaxPool1d-9            [-1, 64, 62499]               0\n",
      "           Conv1d-10           [-1, 128, 62499]          24,704\n",
      "             ReLU-11           [-1, 128, 62499]               0\n",
      "        MaxPool1d-12           [-1, 128, 31249]               0\n",
      "           Conv1d-13            [-1, 64, 31249]          24,640\n",
      "             ReLU-14            [-1, 64, 31249]               0\n",
      "        MaxPool1d-15            [-1, 64, 15624]               0\n",
      "AdaptiveAvgPool1d-16                [-1, 64, 7]               0\n",
      "           Linear-17                 [-1, 1280]         574,720\n",
      "             ReLU-18                 [-1, 1280]               0\n",
      "           Linear-19                  [-1, 256]         327,936\n",
      "             ReLU-20                  [-1, 256]               0\n",
      "           Linear-21                    [-1, 1]             257\n",
      "          Sigmoid-22                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 960,097\n",
      "Trainable params: 960,097\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.91\n",
      "Forward/backward pass size (MB): 648.52\n",
      "Params size (MB): 3.66\n",
      "Estimated Total Size (MB): 654.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "### model\n",
    "class SimpleVGG(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(SimpleVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(7)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7, 128*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128*10, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Sigmoid()        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleVGG()\n",
    "summary(model, (1, 499998))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5de7ea0-1a30-412c-be74-3fa379d9cb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e8284c327b4eccbe018edf007b829a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1462, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [2/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [3/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [4/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [5/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [6/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [7/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [8/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [9/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n",
      "Epoch [10/10], Loss: 0.0000, Training Accuracy: 1.0000\n",
      "Validation Loss: 0.0000, Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = SimpleVGG(num_classes=1)\n",
    "criterion = nn.BCELoss()  # Using Binary Cross Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            outputs = outputs.view(-1)  # Ensure outputs is of shape (batch_size,)\n",
    "            labels = labels.float()  # Ensure labels is of shape (batch_size,)\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            predicted = (outputs > 0.5).long()\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Evaluation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.view(-1)  # Ensure outputs is of shape (batch_size,)\n",
    "                labels = labels.float()  # Ensure labels is of shape (batch_size,)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                predicted = (outputs > 0.5).long()\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = running_val_loss / len(test_dataloader.dataset)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "# Use the training function\n",
    "train(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

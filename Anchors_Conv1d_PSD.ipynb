{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "089cfd87-7de7-4876-9701-931d06a7ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 directories and 0 file in C:\\Users\\patry\\OneDrive\\Pulpit\\kopalnia\n",
      "There are 0 directories and 112 file in C:\\Users\\patry\\OneDrive\\Pulpit\\kopalnia\\020224wieliczka\n",
      "There are 0 directories and 76 file in C:\\Users\\patry\\OneDrive\\Pulpit\\kopalnia\\210224wieliczka\n",
      "There are 0 directories and 86 file in C:\\Users\\patry\\OneDrive\\Pulpit\\kopalnia\\220424wieliczka\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nptdms import TdmsFile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the directory containing TDMS files\n",
    "tdms_dir = r\"C:\\Users\\patry\\OneDrive\\Pulpit\\kopalnia\"\n",
    "\n",
    "# List to store paths of TDMS files\n",
    "tdms_list = []\n",
    "\n",
    "# Walk through the directory and its subdirectories\n",
    "for root, dirs, files in os.walk(tdms_dir):\n",
    "    print(f\"There are {len(dirs)} directories and {len(files)} file in {root}\")\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".tdms\"):\n",
    "            # If the file ends with '.tdms', add its full path to the list\n",
    "            tdms_list.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3bbf1c32-2c55-49ab-b5f6-9928dc913ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store data\n",
    "data_list = []\n",
    "\n",
    "# Open tdms files and create a list with labels\n",
    "for tdms_file_path in tdms_list:\n",
    "    tdms_file = TdmsFile.read(tdms_file_path)\n",
    "    first_letter = os.path.basename(tdms_file_path)[0]\n",
    "    \n",
    "    for group in tdms_file.groups():\n",
    "        for channel in group.channels():\n",
    "            data_len = np.shape(channel.data)[0]\n",
    "            # Check conditions and append data to list\n",
    "            if (data_len <= 1000000 and data_len >= 10000 and first_letter != 'z'):\n",
    "                data_list.append({\n",
    "                    #'anchor_ids': group.name,\n",
    "                    'class': first_letter,\n",
    "                    #'driveway': channel.name,\n",
    "                    'excitation': channel.data,\n",
    "                    'type_id': 1 if first_letter == 'd' else 0\n",
    "                })\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d71127cc-be21-41e5-b00a-c63fab486269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df.loc[df.index % 2 == 0]\n",
    "df_magitude = df.loc[df.index % 2 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16daa4ea-b4e0-45f0-ae50-41d677de5a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>excitation</th>\n",
       "      <th>type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d</td>\n",
       "      <td>[10.325520382999999, 10.325520382999999, 10.32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>[-10.380197332, -8.734103042, -5.822633007, 10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d</td>\n",
       "      <td>[10.325520382999999, 10.325520382999999, -10.3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d</td>\n",
       "      <td>[10.325520382999999, -10.380197332, -10.380197...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d</td>\n",
       "      <td>[-0.02696687200000001, 0.002660044999999999, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                         excitation  type_id\n",
       "1     d  [10.325520382999999, 10.325520382999999, 10.32...        1\n",
       "3     d  [-10.380197332, -8.734103042, -5.822633007, 10...        1\n",
       "5     d  [10.325520382999999, 10.325520382999999, -10.3...        1\n",
       "7     d  [10.325520382999999, -10.380197332, -10.380197...        1\n",
       "9     d  [-0.02696687200000001, 0.002660044999999999, 0...        1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_magitude.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "093462d5-6db0-441c-9da3-5f1695d055ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "def signal_to_psd(signal: np.array, sampling_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Computes the Power Spectral Density (PSD) using Welch's method for the given signal.\n",
    "\n",
    "    Parameters:\n",
    "    signal (np.array): Input signal.\n",
    "    sampling_rate (float): Sampling frequency of the signal (samples per second).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the PSD result and corresponding frequencies.\n",
    "    \"\"\"\n",
    "    frequencies, psd = welch(signal, fs=sampling_rate)\n",
    "    return frequencies, psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a96d2089-8e89-4d78-a7ff-6661f2aede34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum length of the signals\n",
    "max_length = max(df_magitude['excitation'].apply(len))\n",
    "\n",
    "# Zero-pad each signal to the maximum length\n",
    "padded_signals = df_magitude['excitation'].apply(lambda x: np.pad(x, (0, max_length - len(x)), 'constant'))\n",
    "\n",
    "# Initialize the new DataFrame\n",
    "df_fft = pd.DataFrame()\n",
    "\n",
    "# Copy the 'type_id' column from df_magitude\n",
    "df_fft['type_id'] = df_magitude['type_id']\n",
    "\n",
    "# Initialize empty lists to store the frequencies and magnitudes\n",
    "freq_list = []\n",
    "magni_list = []\n",
    "\n",
    "# Compute FFT for each zero-padded signal\n",
    "for signal in padded_signals:\n",
    "    freq, magni = signal_to_psd(np.array(signal), 40000)  # Assuming a sampling rate of 1 Hz\n",
    "    freq_list.append(freq)\n",
    "    magni_list.append(magni)\n",
    "\n",
    "# Add the computed frequencies and magnitudes to the new DataFrame\n",
    "df_fft['frequencies'] = freq_list\n",
    "df_fft['magnitudes'] = magni_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf6152a7-c034-40c2-901b-097ee9a50302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Custom dataset class\n",
    "class Magintude_dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        psd_magnitude = self.df['magnitudes'].iloc[idx]\n",
    "        type_id = self.df['type_id'].iloc[idx]\n",
    "\n",
    "        psd_magnitude = torch.tensor(psd_magnitude, dtype = torch.float32).unsqueeze(dim=0)\n",
    "        type_id = torch.tensor(type_id, dtype = torch.long)\n",
    "\n",
    "        return psd_magnitude, type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a41af5a3-26c7-4393-91c0-2bbbefdbbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split data to train and test\n",
    "train_df, test_df = train_test_split(df_fft, test_size=0.1, random_state = 42)\n",
    "\n",
    "# Make train and test instance of class\n",
    "train_dataset = Magintude_dataset(train_df)\n",
    "test_dataset = Magintude_dataset(test_df)\n",
    "\n",
    "# Make DataLoader\n",
    "BATCH_SIZE  = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d1f06fcc-3bc8-4eda-a662-322665ec3227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 129]), torch.Size([8]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].size(), next(iter(train_dataloader))[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "05aed5ea-4fa7-44c5-ab97-a951a1ba0e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57da4c96-e442-4f7e-a45c-38f1aa3b0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class SimpleVGG(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(SimpleVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(7)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 7, 128*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128*10, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.ReLU()        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "class conv_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_net, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1d(x)\n",
    "\"\"\"        \n",
    "# Create an instance of the model\n",
    "model = SimpleVGG()\n",
    "\n",
    "# Pass the instance to summary\n",
    "#summary(model, (1, 129))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e72c670c-1044-448b-beed-42db009aca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "#pred = model(train_dataset[0][0])\n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3ba59658-35ed-45fe-b118-c38c3e98071f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu' if torch.cuda.is_available else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8d4ece42-6297-41f4-a8d7-1c67d6cc8482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 62.96296296296296\n",
      "Epoch [2/10], Loss: 62.96296296296296\n",
      "Epoch [3/10], Loss: 62.96296296296296\n",
      "Epoch [4/10], Loss: 62.96296296296296\n",
      "Epoch [5/10], Loss: 62.96296296296296\n",
      "Epoch [6/10], Loss: 62.96296296296296\n",
      "Epoch [7/10], Loss: 62.96296296296296\n",
      "Epoch [8/10], Loss: 62.96296296296296\n",
      "Epoch [9/10], Loss: 62.96296296296296\n",
      "Epoch [10/10], Loss: 62.96296296296296\n",
      "Accuracy on test data: 0.25\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Definicja funkcji straty i optymalizatora\n",
    "criterion = nn.BCELoss()  # Używamy Binary Cross Entropy Loss dla binarnej klasyfikacji\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Przygotowanie danych\n",
    "# Załóżmy, że masz dane treningowe i testowe w formie DataLoaderów o nazwach train_loader i test_loader.\n",
    "\n",
    "# Pętla ucząca\n",
    "def train(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Ustawienie modelu w tryb treningu\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()  # Wyzerowanie gradientów\n",
    "            outputs = model(inputs)  # Przekazanie danych przez model\n",
    "            loss = criterion(outputs.squeeze(), labels.float())  # Obliczenie funkcji straty\n",
    "            loss.backward()  # Propagacja wsteczna\n",
    "            optimizer.step()  # Aktualizacja wag\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "\n",
    "    # Ocenianie modelu na danych testowych\n",
    "    model.eval()  # Ustawienie modelu w tryb ewaluacji\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()  # Zaokrąglenie do najbliższej całkowitej wartości i usunięcie wymiaru zbędnego\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy on test data: {accuracy}\")\n",
    "# Użycie funkcji uczącej\n",
    "train(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdaa69a-9f2c-4a61-852d-236ae95ab935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
